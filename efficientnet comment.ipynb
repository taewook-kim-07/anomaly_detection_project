{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1QyD54OkHgdovexLpQN7799yQsFDq0v-A","timestamp":1665466818065}],"machine_shape":"hm","collapsed_sections":[],"mount_file_id":"1QyD54OkHgdovexLpQN7799yQsFDq0v-A","authorship_tag":"ABX9TyPiamOgmMjbJmxAi/dn4AQA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# tensorflow 2.8.0 GUP사용, addons tfa사용\n","!pip install -q tensorflow==2.8.0 tensorflow-addons==0.16.1 numpy pandas \\\n","    scikit-learn h5py Pillow keras-cv-attention-models"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u3F4SmVHsMoM","executionInfo":{"status":"ok","timestamp":1664428928700,"user_tz":-540,"elapsed":62249,"user":{"displayName":"이도현","userId":"11348764420557844160"}},"outputId":"dd7e3757-088e-49f4-8927-05adcb19edfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 668.3 MB 17 kB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 6.5 MB/s \n","\u001b[K     |████████████████████████████████| 508 kB 77.3 MB/s \n","\u001b[K     |████████████████████████████████| 462 kB 74.4 MB/s \n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["import"],"metadata":{"id":"4yDbgG2g1Mff"}},{"cell_type":"code","source":["import os #파일 경로 폴더를 처리\n","import glob #검색조건에 해당하는 파일명 가지고 온다\n","import math #연산\n","import random #랜덤\n","import collections #빈도를 알아낸다\n","import numpy as np\n","import pandas as pd\n","from PIL import Image #이미지 처리 pillow\n","\n","from sklearn.model_selection import StratifiedKFold #imbalanceset의 train:validation 비를 감안하여 나누는 것\n","\n","from sklearn.preprocessing import LabelEncoder #명목형을 숫자로\n","import tensorflow as tf \n","print('tf:', tf.__version__)\n","import tensorflow_addons as tfa #최신버전 연산,손실,옵티마이저 등 계산\n","print('tfa:', tfa.__version__)\n","from keras_cv_attention_models import efficientnet #사용할 모델"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1e3dbcxJsPdN","executionInfo":{"status":"ok","timestamp":1664429013306,"user_tz":-540,"elapsed":3179,"user":{"displayName":"이도현","userId":"11348764420557844160"}},"outputId":"c5febe3c-ef2a-4bf8-d1a5-669c7e351b1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf: 2.8.0\n","tfa: 0.16.1\n"]}]},{"cell_type":"markdown","source":["settings"],"metadata":{"id":"T2n4Gnsp1QCP"}},{"cell_type":"code","source":["#args는 인수를 의미\n","class args:\n","    data_dir = os.path.join(os.path.expanduser('~'), 'solution') # Directory containing CSV files, directory는 folder의 상위개념으로 파일의 저장소를 directory로 지정한다, 초기 경로 구성 요소 ~에서 홈 디렉토리로 확장 ~의 경우 home의 환경변수로대체\n","    data_tfrec_dir = os.path.join(os.path.expanduser('~'), 'solution/tfrec') # 디랙토리가 tfrec를 포함\n","    data_tfrec_test  = os.path.join(data_tfrec_dir, 'test.tfrecord*') # test에 wildcard = 한번에 명령\n","    data_preds_dir   = 'preds'         # 예측을 저장\n","    seed             = 31542           # 시드설정으로 난수 예측\n","    mixed_precision  = 'mixed_float16' # 속도와 메모리에서 이(메모리 절반),tensorflow제공\n","    job              = 'train_test'    # 작업할 내용 = train test\n","    n_folds          = 5               # fold 수, StratifiedKFold에 쓰이는 것으로 fold=5는 4:1=train:test를 의미 이때 비율을 유지한다\n","    n_channels       = 3               # rgb\n","    dim              = 512             # 픽셀크기\n","    n_examples_train = 3422            # 전체 트레인 중 4/5\n","    n_epochs         = 200             # epoch가 커도 된다. early stop\n","    batch_size       = 32              # Batch size: 32 for 4x GPU, 64 for TPUv2\n","    lr               = 1e-4            # Learning rate\n","    weights          = 'imagenet21k'   # 여기서 weights를 가지고 옴\n","    aug_ratio        = 0.9             # 총 증강 수와 상관 없이 증강이 이미지가 출력될 확률\n","    n_aug            = -1              # train의 증강되는 숫자 -1은 모두\n","    n_tta            = 1               # test의 증강되는 숫자 1은 안함\n","    n_classes        = 88              # 88개 카테고리\n","    class_weight     = None            # 각 클래스에 지정하지 않음\n","    create_tfrecords = True            # 나중에 makedirs에 쓰이는데 다수의 디렉토리 설정함\n","   \n","    # exist_ok=True로 설정하면 파일이 없을 시에 생성 있으면 그냥 넘어감, 아래에 종종 나옴\n","    \n","# Check max folds, assert는 뒤조건이 아니면 error가 나버림\n","assert args.n_folds <= 10, 'Max 10 folds supported'"],"metadata":{"id":"ryAn2yTosygZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["definitions"],"metadata":{"id":"tj_HHOfm1Sn7"}},{"cell_type":"code","source":["    #환경변수는 시스템에서 지정해주는 변수를 의미, os.environ[]은 안의 변수값을 불러온다\n","    #환경변수는 컴퓨터 작동에서 필요한 값을 의미하므로 건들때 조심해야한다\n","    #동일한 양상으로 변수를 설정함 = set seed\n","    #아래에 _=seeder(args.seed+fold_id)를 쓰긴 함\n","def seeder(seed):\n","    \"\"\"Set seed\"\"\"\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","    \n","    return seed\n","    \n","\n","    #hyperparameter(설정값)을 모두 list로 보겠다.\n","    #hyperparameter는 모델의 외적인 요소, 모델링 경험으로 찾아가는 것이다.lr와 같이 사용자가 설정\n","    #parameter는 머신러닝 인자이다. 최적화된 값을 목표로 한다.가중치, 결정계수, 벡터값과 같이\n","    #parameter는 함수에서 매개변수로 설정되는 값이라면 argument는 함수에 전달하는 인자이다.\n","    #vars는 객체의 dictionary를 반환 \n","    #dictionary는 자료형의 인덱스를 가질수 있는 list\n","    #딕셔너리에 대한 값을 리스트로 반환하되 설정을 넣음\n","    #def train_predict()에서 쓰이는데 일단 패스\n","def lsargs():\n","    \"\"\"List all hyperparameters\"\"\"\n","    for a in sorted([a for a in vars(args) if '__' not in a]): \n","        print('%-20s %s' % (a, vars(args)[a]))\n","\n","\n","#fold=5 -> train을 train:validation = 4:1로 5번 학습한다. 가장 작은 label이 5개이므로 6이상은 불가하다.\n","def create_cv_split(data_dir, n_splits=5):\n","    \"\"\"Read .csv files and create 5-fold split\"\"\"\n","\n","    #train은 공유폴더에 사진+csv로 존재한다.\n","    #test를 따로 받음, 엑셀로 정리된 파일 이미지는 별도인듯하다    \n","    train_df = pd.read_csv(os.path.join(data_dir, 'train_df.csv'))\n","    test_df = pd.read_csv(os.path.join(data_dir, 'test_df.csv'))\n","\n","    #test label을 unknown으로 한다 \n","    test_df['label'] = 'unk'\n","\n","    #train과test 파일이 동일 위치에 있어야 한다.\n","    train_df['image'] = data_dir + '/train/' + train_df['file_name']\n","    test_df['image']  = data_dir + '/test/' + test_df['file_name']\n","\n","    #전처리에서 명목형 자료 데이터를 숫자로 치환 \n","    le = LabelEncoder()\n","\n","    #wood-good과 같은 label을 숫자로 변환, 총 88개가 나올 것이다.\n","    train_df['label_int'] = le.fit_transform(train_df['label'])\n","\n","    #값을 넣기 위해 열 추가 \n","    test_df['label_int'] = 0\n","    train_df['fold_id'] = 0\n","    test_df['fold_id'] = 0\n","\n","    #데이터분할할건데 4:1로 무작위 추출을 33의 버전으로 나눈다\n","    #kfold=5인 이유 가장 적은 label의 값이 5이기 때문\n","    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=33)\n","\n","    # train과 train의 label의 values(값을 의미)를 비율분할해서 인덱스와 값을 가지고 온다. fold_id가 index\n","    #train과 val(validation)을 고정학습하면 val이 파라미터튜닝되어 과적합이 된다.\n","    #이를 해결하려고 교차검증하는데 모든 데이터를 활용하면서 과적합을 방지하고 시간이 k배로 늘어난다\n","    #교차검정이 앙상블도 관련이 있다.\n","    #하이퍼파라미터 튜닝기법 = lr, cost, 정규화파라미터, 가중치초기화 등등\n","    #sample(frac=1.0)은 데이터프레임에서 특정비율의 무작위 표본 추출한다.(0~1)\n","    #1은 모두 추출한다는 것이다 0.5는 4개중 2개 \n","    for fold_id, (train_index, val_index) in enumerate(kf.split(train_df, train_df['label'].values)):\n","        train_df.loc[train_df.index.isin(val_index), 'fold_id'] = fold_id\n","    train_df = train_df.sample(frac=1.0, random_state=34)\n","    return train_df, test_df\n","\n","\n","    #tfrecold는 tensorflow에서 자료구조형, 빠른 입출력, 데이터의 직렬화를 담당한다.\n","    #tfrecord는 파일정보와 label을 한번에 관리해서 빠르고 직관적이다.\n","    #이미지는 라벨이 따로 저장되어 읽기에 코드가 복잡하다.\n","    # 데이터프레임에 들어가는 파일로 class를 만들 예정이다.\n","    #이미지 정보와 id, 라벨과 id를 4가지를 읽어서 한번에 저장하는 것\n","class TFRecordProcessor(object):\n","    \"\"\"Class to create TFRecord files using input from a DataFrame\"\"\"\n","        self.n_examples = 0\n","        \n","    # class지정은 여러대의 계산기를 한 컴퓨터에서 돌리는 것을 의미한다.\n","    #__init__(self)은 앞을 초기화 하고 실행한다.\n","    #반복문을 돌리기위해 비우는 느낌\n","    def __init__(self):\n","\n","      # BytesList won't unpack a string from an EagerTensor.\n","      #isinstance는 value가 타입이 맞으면 true를 보낸다.\n","      #tf.constant(0)는 변수를 선언한다.0의 type가 int이므로\n","      #tf.train.bytestlist은 문자열 타입 데이터의 객체이다.\n","      #tf. int64는 bool, int, uint를 담당한다\n","      #tf. float는 float32, 64를 담당한다.     \n","      #어떤 데이터 형이 와도 그거 저장해서 각 데이터형으로 리턴하겠다.ex) bytes는 string/byte로 받아서 byte로 리턴\n","    def _bytes_feature(self, value):\n","        if isinstance(value, type(tf.constant(0))):\n","            value = value.numpy() \n","        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n","    def _int_feature(self, value):\n","        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n","    def _float_feature(self, value):\n","        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n","\n","    def _process_example(self, ind, A, B, C, D):\n","        self.n_examples += 1\n","        feature = collections.OrderedDict() #ordereddict데이터를 순서대로 받는 역할을 한다.\n","        feature['image_id'] = self._bytes_feature(A[ind].encode('utf-8'))\n","        feature['image'] =    self._bytes_feature(tf.io.read_file(B[ind]))\n","        feature['label_id'] = self._bytes_feature(C[ind].encode('utf-8'))\n","        feature['label'] =    self._int_feature(D[ind])\n","\n","        #encoder는 10진수를 2진수로 암호화 decoder는 2진수를 10진수로 복원\n","        #uft-8은 가변 길이 유니코드 인코딩 4바이트이다. 널리쓰임\n","        #uft-16은 사용하는 것보다 더 많은 바이트를 소모한다.\n","        #tf.train.example은 {'string':tf.train.feature}의 매핑\n","        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n","        self._writer.write(example_proto.SerializeToString())\n","\n","    #\n","    def write_tfrecords(self, A, B, C, D, n_shards=1, file_out='train.tfrecord'):\n","        n_examples_per_shard = A.shape[0] // n_shards\n","        n_examples_remainder = A.shape[0] %  n_shards\n","        self.n_examples = 0\n","        for shard in range(n_shards):\n","            self._writer = tf.io.TFRecordWriter('%s-%05d-of-%05d' % (file_out, shard, n_shards))\n","            start = shard * n_examples_per_shard\n","            if shard == (n_shards - 1):\n","                end = (shard + 1) * n_examples_per_shard + n_examples_remainder\n","            else:\n","                end = (shard + 1) * n_examples_per_shard\n","            print('Shard %d of %d: (%d examples)' % (shard, n_shards, (end - start)))\n","            for i in range(start, end):\n","                self._process_example(i, A, B, C, D)\n","                print(i, end='\\r')\n","            self._writer.close()\n","        return self.n_examples\n","        \n","\n","# TPU에 접근해서 학습을 하겠다.\n","def init_tpu(tpu=None):\n","    \"\"\" Seamlessly init any accelerator: CPU, GPU, multi-GPU, TPU\n","    \"\"\"\n","    try:\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=tpu)\n","        strategy = tf.distribute.TPUStrategy(tpu)\n","        print('[init_tpu] Master:      ', tpu.master())\n","    except ValueError:\n","        strategy = tf.distribute.MirroredStrategy() #기존 모델의 코드를 조금만 바꾸어 분산 훈련 진행 여러 tpu로 분할\n","        print('[init_tpu] TPU was not found')\n","    print('[init_tpu] Num replicas:', strategy.num_replicas_in_sync)\n","    return strategy\n","\n","\n","    #deterministic 결정론적 어떤 입력이든 동일한 과정, 결과로\n","    #tta는 test time augmentation으로 test할때 ensemble 느낌, buffer_size는 이름이 들어가는 칸, file_glob는 파일 경로로 불러온다,cache 빨리 불러오는 것\n","    #이미지를 불러옴\n","    #ds.map key와 값을 저장하기 유용한 데이터 구조\n","    #classback은 인자로 넘겨지는 함수를 콜백하는 함수. 특정조건에 호출\n","    #ckpt는 모델 가중치 체크포인트 파일(가중치만 있다 구조없다) 메타정보로 재학습가능하나 무겁다.\n","    #그외 pb(가중치+구조), pbtxt(pb의 txt=모델구조)있다.\n","    #repeat는 한번 더 실행한다.\n","    #drop_remainder는 true일때 batch 남는거 버림, false는 돌림\n","    #속도를 빠르게 하기위해 파이프라인으로 데이터를 가지고 온다 prefatch\n","def init_tfdata(files_glob, deterministic=True, batch_size=32, auto=-1, \n","                parse_example=None, aug=None, tta=None, norm=None, \n","                repeat=False, buffer_size=None, cache=False, drop_remainder=False):\n","    \"\"\" Creates tf.data.TFRecordDataset with appropriate parameters.\"\"\"\n","\n","    options = tf.data.Options()\n","    options.experimental_deterministic = deterministic\n","    files = tf.data.Dataset.list_files(files_glob, \n","        shuffle=not deterministic).with_options(options) #not deterministic이란 같은 입력이라도 매번 다른 결과\n","    print('N tfrec files:', len(files))\n","    #\n","    ds = tf.data.TFRecordDataset(files, num_parallel_reads=auto)\n","    ds = ds.with_options(options)\n","    ds = ds.map(parse_example, num_parallel_calls=auto)\n","    #\n","    if aug:\n","        ds = ds.map(aug, num_parallel_calls=auto)\n","    if tta:\n","        ds = ds.map(tta, num_parallel_calls=auto)\n","    if norm:\n","        ds = ds.map(norm, num_parallel_calls=auto)\n","    if repeat:\n","        ds = ds.repeat()\n","    if buffer_size:\n","        ds = ds.shuffle(buffer_size=buffer_size, \n","            reshuffle_each_iteration=True)\n","    ds = ds.batch(batch_size=batch_size, \n","        drop_remainder=drop_remainder)\n","    ds = ds.prefetch(auto)\n","    if cache:\n","        ds = ds.cache()\n","    return ds\n","\n","    #wildcard는 여러파일을 한번에 저장하기 위한 것, *은 생략을 의미하는 wildecard 문자다\n","    #파일 용량이 많으니 마지막것을 제외하고 삭제하는 것을 의미 (마지막 가중치만 저장)\n","class KeepLastCKPT(tf.keras.callbacks.Callback):\n","    \"\"\" Sort all ckpt files matching the wildcard \n","    and remove all except last.\n","    \"\"\"\n"," \n","    def __init__(self, wildcard):\n","        super(KeepLastCKPT, self).__init__() #상속을 받는다\n","        self.wildcard = wildcard\n","\n","    #매번 epoch가 시작될 때 호출, epoch는 0으로 시작하는 현재 epoch의 index이다.\n","    def on_epoch_begin(self, epoch, logs=None):\n","        files = sorted(tf.io.gfile.glob(self.wildcard))\n","        if len(files):\n","            for file in files[:-1]:\n","                tf.io.gfile.remove(file)\n","            print('Kept ckpt: %s' % files[-1])\n","        else:\n","            print('No ckpt to keep')\n","\n","    #훈련이 끝날 때 호출\n","    def on_train_end(self, logs=None):\n","        files = sorted(tf.io.gfile.glob(self.wildcard))\n","        if len(files):\n","            for file in files[:-1]:\n","                tf.io.gfile.remove(file)\n","            print('\\nKept ckpt (final): %s' % files[-1])\n","        else:\n","            print('\\nNo ckpt to keep (final)')\n","\n","# 간단한 정보를 보여주는 description\n","feature_description = {\n","    'image':    tf.io.FixedLenFeature([], tf.string),\n","    'label':    tf.io.FixedLenFeature([], tf.int64),\n","}\n","\n","\n","#이미지를 불러온다.\n","#tf.cast\n","def parse_example(example_proto):\n","    \"\"\"Parse example and return image in uint8 dtype\"\"\"\n","    d = tf.io.parse_single_example(example_proto, feature_description)\n","    image = tf.image.decode_jpeg(d['image'], channels=args.n_channels, dct_method='INTEGER_ACCURATE')\n","    image = tf.image.resize(image, [args.dim, args.dim])\n","    image = tf.reshape(image, [args.dim, args.dim, args.n_channels])\n","    image = tf.cast(image, tf.uint8)\n","    label = tf.cast(d['label'], tf.int32)\n","    return image, label\n","\n","\n","# List of augmentation functions\n","aug_func_list = [\n","    # x90 positions of orig image\n","    lambda image, label: (image, label),\n","    lambda image, label: (tfa.image.rotate(image, math.pi/180*90 ), label),\n","    lambda image, label: (tfa.image.rotate(image, math.pi/180*180), label),\n","    lambda image, label: (tfa.image.rotate(image, math.pi/180*270), label),\n","    # x90 positions of flipped-left-right image\n","    lambda image, label: (tf.image.flip_left_right(image), label),\n","    lambda image, label: (tf.image.transpose(image), label),\n","    lambda image, label: (tf.image.flip_up_down(image), label),\n","    lambda image, label: (tfa.image.rotate(tf.image.flip_up_down(image), math.pi/180*90), label),\n","    # x45 positions of orig image without central crop, fill const 0 (black)\n","    lambda image, label: (tfa.image.rotate(image, math.pi/180*45 ), label),\n","    lambda image, label: (tfa.image.rotate(image, math.pi/180*135), label),\n","    lambda image, label: (tfa.image.rotate(image, math.pi/180*225), label),\n","    lambda image, label: (tfa.image.rotate(image, math.pi/180*315), label),\n","]\n","\n","# -1 means all augs, 0 means only aug_func_list[0] which does nothing\n","if args.n_aug == -1:\n","    args.n_aug = len(aug_func_list)\n","aug_func_list = aug_func_list[:args.n_aug]\n","\n","# Only functions used for aug may be used for tta. \n","# Separate list allows amount of tta to be less or equal to amount of aug\n","if args.n_tta == -1:\n","    args.n_tta = len(aug_func_list)\n","tta_func_list = aug_func_list[:args.n_tta]\n","\n","\n","class Lambda:\n","    \"\"\"\n","    Callable class used to create unified augmentation functions for `tf.switch_case`\n","    \"\"\"\n","    def __init__(self, func, image, label):\n","        self._func = func\n","        self._image = image\n","        self._label = label\n","    def __call__(self):\n","        return self._func(self._image, self._label)\n","\n","\n","#minval<= <maxval 범위\n","#aug가 12 범위가 0~11로 같은 갯수를 가진다.\n","def aug(image, label):\n","    \"\"\"\n","    Run single transformation per call with `aug_ratio` probability\n","    \"\"\"\n","    if args.n_aug > 1:\n","        image, label = tf.switch_case(\n","            branch_index=tf.random.uniform(\n","                shape=[],       \n","                minval=0, \n","                maxval=round((args.n_aug-1)/args.aug_ratio), dtype=tf.int32),\n","            branch_fns=[Lambda(func, image, label) for func in aug_func_list],\n","            default=Lambda(aug_func_list[0], image, label),)\n","    return image, label\n","\n","\n","def norm(image, label):\n","    \"\"\"Normalization\"\"\"\n","    image = tf.image.resize(image, [args.dim, args.dim])\n","    image = tf.reshape(image, [args.dim, args.dim, args.n_channels])\n","    image = tf.cast(image, tf.float32)\n","    label = tf.cast(tf.one_hot(label, args.n_classes), tf.int32)\n","    image = image / 255.0\n","    return image, label\n","\n","\n","def init_model(print_summary=True):\n","    \"\"\"Initialize a model\"\"\"\n","    model = tf.keras.Sequential([\n","        efficientnet.EfficientNetV2L(\n","            input_shape=(args.dim, args.dim, args.n_channels), \n","            pretrained=args.weights, \n","            num_classes=0),\n","        tf.keras.layers.GlobalAveragePooling2D(),\n","        tf.keras.layers.Dense(args.n_classes, activation='softmax')\n","    ], name='model')\n","    model.compile(optimizer=tf.keras.optimizers.Adam(args.lr), \n","                  loss=tf.keras.losses.CategoricalCrossentropy(),\n","                  metrics=[tfa.metrics.F1Score(num_classes=args.n_classes, \n","                                               average='macro', \n","                                               name='f1')])\n","    if print_summary:\n","        model.summary()\n","    return model\n","\n","\n","#학습\n","#설정값을 보는 코드\n","def train_predict():\n","    \"\"\"Run training and prediction in each of 5 folds\"\"\"\n","    lsargs()\n","    for fold_id in range(0, args.n_folds):\n","        print('\\n*****')\n","        print('Fold:', fold_id)\n","        print('*****\\n')\n","        #--------------------------------------------------------------------------\n","        print('Clear session...')\n","        tf.keras.backend.clear_session() #캐쉬를 지운다\n","        #--------------------------------------------------------------------------\n","        print('Set fold-specific seed...')\n","        _ = seeder(args.seed + fold_id)\n","        #--------------------------------------------------------------------------\n","        #정밀도와 속도의 trade-off, 부동소수점\n","        #mixed_precision은 10진수를 2진수로 바꾼다(부동소수점)\n","        #메모리를 빌려서 32->16 빠르게 메모리 덜 소모하며 학습한다.        \n","        if args.mixed_precision is not None:\n","            print('Init Mixed Precision:', args.mixed_precision)\n","            policy = tf.keras.mixed_precision.experimental.Policy(args.mixed_precision)\n","            tf.keras.mixed_precision.experimental.set_policy(policy)\n"," \n","        else:\n","            print('Using default precision:', tf.keras.backend.floatx())\n","        #--------------------------------------------------------------------------\n","        print('FULL BATCH SHAPE: %d x %d x %d x %d' % (args.batch_size,\n","                                                       args.dim,\n","                                                       args.dim,\n","                                                       args.n_channels))\n","        #--------------------------------------------------------------------------\n","        print('N aug (including original):', len(aug_func_list))\n","        print('N tta (including original):', len(tta_func_list))\n","        #--------------------------------------------------------------------------\n","        # Init TPU\n","        print('Init TPU')\n","        strategy = init_tpu()\n","        #--------------------------------------------------------------------------\n","        # 파일을 불러와서 tfrec에 넣는다\n","        # Globs\n","        all_fold_ids = np.array(range(args.n_folds))\n","        train_fold_ids = all_fold_ids[all_fold_ids != fold_id]\n","        train_glob = os.path.join(\n","            args.data_tfrec_dir, \n","            ('fold.[' + '%d'*(args.n_folds-1) + '].tfrecord*') % tuple(train_fold_ids))\n","        val_glob   = os.path.join(args.data_tfrec_dir, 'fold.[%d].tfrecord*' % fold_id)\n","        print('TRAIN GLOB:', train_glob)\n","        print('VAL   GLOB:', val_glob)\n","        print('TEST  GLOB:', args.data_tfrec_test)\n","        #--------------------------------------------------------------------------\n","        #\n","        print('Init datasets')\n","        train_ds = init_tfdata(train_glob, \n","                               deterministic=False,  \n","                               batch_size=args.batch_size, \n","                               auto=-1,\n","                               parse_example=parse_example, \n","                               aug=aug, \n","                               norm=norm,\n","                               repeat=True,\n","                               buffer_size=256, \n","                               cache=False)\n","        val_ds = init_tfdata(val_glob, \n","                             deterministic=True,  \n","                             batch_size=args.batch_size * 2, \n","                             auto=-1,\n","                             parse_example=parse_example,\n","                             norm=norm,\n","                             repeat=False,  \n","                             cache=True)\n","        #--------------------------------------------------------------------------\n","        print('Init model')\n","        with strategy.scope():\n","            model = init_model(print_summary=True)\n","        #--------------------------------------------------------------------------\n","        print('Init callbacks')\n","        call_ckpt = tf.keras.callbacks.ModelCheckpoint(\n","            'model-f%d-e{epoch:03d}-{val_loss:.4f}-{val_%s:.4f}.h5' % (fold_id, 'f1'),\n","            monitor='val_f1',\n","            save_best_only=True,\n","            save_weights_only=True,\n","            mode='max',\n","            verbose=1)\n","        call_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n","            monitor='val_f1', \n","            factor=0.5, \n","            patience=4, \n","            min_delta=1e-4,\n","            min_lr=1e-8,\n","            verbose=1,\n","            mode='max')\n","        #early_stop을 지시, 불필요하게 gpu와 시간이 소비되는 것을 막는다.\n","        #8번안에 1e-4차이가 안나면 멈춘다\n","        call_early_stop = tf.keras.callbacks.EarlyStopping(\n","            monitor='val_f1',\n","            patience=8,\n","            min_delta=1e-4,\n","            mode='max',\n","            verbose=1)\n","        call_keep_last = KeepLastCKPT(wildcard='model-f%d-e*.h5' % fold_id)\n","        #-------------------------------------------------------------------------- \n","        if 'train' in args.job:\n","            print('Fit (fold %d)' % fold_id)\n","            h = model.fit(\n","                train_ds,\n","                steps_per_epoch=args.n_examples_train // args.batch_size,\n","                epochs=args.n_epochs,\n","                validation_data=val_ds,\n","                callbacks=[call_ckpt,\n","                           call_reduce_lr,\n","                           call_early_stop,\n","                           call_keep_last],\n","                class_weight=args.class_weight,\n","            )\n","        #--------------------------------------------------------------------------\n","        #마지막게 아니라 제일 좋은 것을 불러온다\n","        m = sorted(glob.glob('model-f%d*.h5' % fold_id))[-1]\n","        print('Load model (fold %d): %s' % (fold_id, m))\n","        model.load_weights(m)\n","        #--------------------------------------------------------------------------\n","        # Predict\n","        #--------------------------------------------------------------------------\n","        for tta_id in range(len(tta_func_list)):\n","            print('Init datasets for prediction (fold %d, tta %d)' % (fold_id, tta_id))\n","            test_ds = init_tfdata(args.data_tfrec_test, \n","                                  deterministic=True,  \n","                                  batch_size=args.batch_size * 2, \n","                                  auto=-1,\n","                                  parse_example=parse_example,\n","                                  tta=tta_func_list[tta_id],\n","                                  norm=norm,\n","                                  repeat=False,\n","                                  cache=True)\n","            #--------------------------------------------------------------------------\n","            #앙상블하기위해 제일 좋은것을 배열로 저장\n","            if 'test' in args.job:\n","                print('Predict TEST (fold %d, tta %d)' % (fold_id, tta_id))\n","                y_pred_test = model.predict(test_ds, verbose=1)\n","                np.save(os.path.join(args.data_preds_dir, 'y_pred_test_fold_%d_tta_%d.npy' % (fold_id, tta_id)), y_pred_test)\n","            #--------------------------------------------------------------------------\n","            \n","\n","#여러개의 모델을 평균해서 가장 큰 것을 가져온다\n","def ensemble():\n","    \"\"\"Compute ensemble, apply debiasing, and save submission\"\"\"\n","    files = sorted(glob.glob(os.path.join(args.data_dir, 'model_*', 'preds', '*.npy')))\n","    probas = []\n","    for file in files:\n","        probas.append(np.load(file))\n","    \n","    probas = np.mean(probas, axis=0)\n","    label_top_2 = np.argsort(probas, axis=1)[:, ::-1][:, :2]\n","    probas_top_2 = np.sort(probas, axis=1)[:, ::-1][:, :2]\n","    \n","    train_df = pd.read_csv(os.path.join(args.data_dir, 'train_df.csv'))\n","    le = LabelEncoder()\n","    le = le.fit(train_df['label'])\n","    \n","    label_inv = []\n","    for col in range(2):\n","        label_inv.append(le.inverse_transform(label_top_2[:, col]))\n","    \n","    subm_df = pd.read_csv(os.path.join(args.data_dir, 'sample_submission.csv'))\n","    subm_df['label'] = label_inv[0]\n","    subm_df['label_2nd'] = label_inv[1]\n","    subm_df['proba'] = probas_top_2[:, 0]\n","    \n","    \n","    #시행착오에 대한 방안\n","    # Debiasing: if \"good\"-class confidence is low - replace with \"defective\"-class\n","    def fn(row):\n","        if 'good' in row['label'] and row['proba'] < 0.70:\n","            return row['label_2nd']\n","        else:\n","            return row['label']\n","    subm_df['label'] = subm_df.apply(fn, axis=1)\n","    subm_df[['index', 'label']].to_csv(os.path.join(args.data_dir, 'submission.csv'), index=False)"],"metadata":{"id":"aidPDkYg8D-v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["create TFRecord dataset"],"metadata":{"id":"3U77Htfh1ZuX"}},{"cell_type":"code","source":["#tfrecord는 tensorflow의 저장 바이너리 데이터 포맷\n","#이미지를 읽을때 메타데이터와 라벨정보를 분리해서 가져오는데 코드가 어렵다\n","#메타데이터는 사진에서 날짜 이미지 파일 크기  등을 의미한다.\n","#이미지는 매던 인코딩/디코딩 작업을 해야한다.\n","if args.create_tfrecords:\n","\n","    os.makedirs(args.data_tfrec_dir, exist_ok=True)\n","\n","    train_df, test_df = create_cv_split(args.data_dir, n_splits=args.n_folds)\n","    \n","    tfrp = TFRecordProcessor()\n","    \n","    # Train\n","    for fold_id in range(len(train_df['fold_id'].unique())):\n","        print('Fold:', fold_id)\n","        n_written = tfrp.write_tfrecords(\n","            train_df[train_df['fold_id'] == fold_id]['file_name'].values,\n","            train_df[train_df['fold_id'] == fold_id]['image'].values,\n","            train_df[train_df['fold_id'] == fold_id]['label'].values,\n","            train_df[train_df['fold_id'] == fold_id]['label_int'].values,\n","            #\n","            n_shards=1, \n","            file_out=os.path.join(args.data_tfrec_dir, 'fold.%d.tfrecord' % fold_id))\n","    \n","    # Test\n","    n_written = tfrp.write_tfrecords(\n","        test_df['file_name'].values,\n","        test_df['image'].values,\n","        test_df['label'].values,\n","        test_df['label_int'].values,\n","        #\n","        n_shards=1,\n","        file_out=os.path.join(args.data_tfrec_dir, 'test.tfrecord'))\n"],"metadata":{"id":"VRapElAv1Y5k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Model-1: train and predict"],"metadata":{"id":"ji_k9yEq1fMO"}},{"cell_type":"code","source":["d = os.path.join(args.data_dir, 'model_1')\n","os.makedirs(d, exist_ok=True)\n","os.chdir(d)\n","os.makedirs(args.data_preds_dir, exist_ok=True)\n","print(os.getcwd())\n","\n","train_predict()"],"metadata":{"id":"UZgl2IhF1fye"},"execution_count":null,"outputs":[]}]}